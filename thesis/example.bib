@ARTICLE{act,
   author = {{Graves}, A.},
    title = "{Adaptive Computation Time for Recurrent Neural Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1603.08983},
 keywords = {Computer Science - Neural and Evolutionary Computing},
     year = 2016,
    month = mar,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160308983G},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{skip-rnn,
   author = {{Campos}, V. and {Jou}, B. and {Giro-i-Nieto}, X. and {Torres}, J. and 
	{Chang}, S.-F.},
    title = "{Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1708.06834},
 primaryClass = "cs.AI",
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
     year = 2017,
    month = aug,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170806834C},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@BOOK{machine-learning,
  TITLE = {Machine Learning},
  AUTHOR = {Tom Mitchell},
  YEAR = {1997},
}

@article{alphago,
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and Driessche, George van den and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  title = {Mastering the game of Go without human knowledge},
  journal = {Nature},
  publisher = {Nature Publishing Group},
  issn = {0028-0836},
  doi = {10.1038/nature24270},
  volume = {550},
  month = {10},
  pages = {354--359},
  number = {7676},
  url = {http:https://doi.org/10.1038/nature24270},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.}
}

@book{deep-learning,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{no-free-lunch,
author = {David H. Wolpert},
title = {The Lack of A Priori Distinctions Between Learning Algorithms},
journal = {Neural Computation},
volume = {8},
number = {7},
pages = {1341-1390},
year = {1996},
doi = {10.1162/neco.1996.8.7.1341},

URL = { 
        https://doi.org/10.1162/neco.1996.8.7.1341
    
},
eprint = { 
        https://doi.org/10.1162/neco.1996.8.7.1341
    
}
,
    abstract = { This is the first of two papers that use off-training set (OTS) error to investigate the assumption-free relationship between learning algorithms. This first paper discusses the senses in which there are no a priori distinctions between learning algorithms. (The second paper discusses the senses in which there are such distinctions.) In this first paper it is shown, loosely speaking, that for any two algorithms A and B, there are “as many” targets (or priors over targets) for which A has lower expected OTS error than B as vice versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is “anti-cross-validation” (choose the learning algorithm with largest cross-validation error). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one cannot say: if empirical misclassification rate is low, the Vapnik-Chervonenkis dimension of your generalizer is small, and the training set is large, then with high probability your OTS error is small. Other implications for “membership queries” algorithms and “punting” algorithms are also discussed. }
}

@article{cauchy1847methode,
  title={M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des systemes d’{\'e}quations simultan{\'e}es},
  year = {1847},
  author={Cauchy, Augustin}
}


@article{rumelhart,
	Author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	Date = {1986/10/09/online},
	Date-Added = {2018-01-15 18:48:42 +0000},
	Date-Modified = {2018-01-15 18:48:42 +0000},
	Day = {09},
	Journal = {Nature},
	L3 = {10.1038/323533a0; },
	Month = {10},
	Pages = {533 EP  -},
	Publisher = {Nature Publishing Group SN  -},
	Title = {Learning representations by back-propagating errors},
	Ty = {JOUR},
	Url = {http://dx.doi.org/10.1038/323533a0},
	Volume = {323},
	Year = {1986},
	Bdsk-Url-1 = {http://dx.doi.org/10.1038/323533a0}}
