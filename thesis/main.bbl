% $ biblatex auxiliary file $
% $ biblatex bbl format version 2.8 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\sortlist[entry]{nyt/global/}
  \entry{skip-rnn}{article}{}
    \name{author}{5}{}{%
      {{hash=CV}{%
         family={{Campos}},
         familyi={C\bibinitperiod},
         given={V.},
         giveni={V\bibinitperiod},
      }}%
      {{hash=JB}{%
         family={{Jou}},
         familyi={J\bibinitperiod},
         given={B.},
         giveni={B\bibinitperiod},
      }}%
      {{hash=GX}{%
         family={{Giro-i-Nieto}},
         familyi={G\bibinitperiod},
         given={X.},
         giveni={X\bibinitperiod},
      }}%
      {{hash=TJ}{%
         family={{Torres}},
         familyi={T\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=CSF}{%
         family={{Chang}},
         familyi={C\bibinitperiod},
         given={S.-F.},
         giveni={S\bibinitperiod-F\bibinitperiod},
      }}%
    }
    \keyw{Computer Science - Artificial Intelligence, Computer Science -
  Computer Vision and Pattern Recognition}
    \strng{namehash}{CV+1}
    \strng{fullhash}{CVJBGXTJCSF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2017}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \verb{eprint}
    \verb 1708.06834
    \endverb
    \field{title}{{Skip RNN: Learning to Skip State Updates in Recurrent Neural
  Networks}}
    \field{journaltitle}{ArXiv e-prints}
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.AI}
    \field{month}{08}
    \field{year}{2017}
  \endentry

  \entry{cauchy1847methode}{article}{}
    \name{author}{1}{}{%
      {{hash=CA}{%
         family={Cauchy},
         familyi={C\bibinitperiod},
         given={Augustin},
         giveni={A\bibinitperiod},
      }}%
    }
    \strng{namehash}{CA1}
    \strng{fullhash}{CA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1847}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{title}{M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des
  systemes d’{\'e}quations simultan{\'e}es}
    \field{year}{1847}
  \endentry

  \entry{deep-learning}{book}{}
    \name{author}{3}{}{%
      {{hash=GI}{%
         family={Goodfellow},
         familyi={G\bibinitperiod},
         given={Ian},
         giveni={I\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Yoshua},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=CA}{%
         family={Courville},
         familyi={C\bibinitperiod},
         given={Aaron},
         giveni={A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {MIT Press}%
    }
    \strng{namehash}{GIBYCA1}
    \strng{fullhash}{GIBYCA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2016}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{note}{\url{http://www.deeplearningbook.org}}
    \field{title}{Deep Learning}
    \field{year}{2016}
  \endentry

  \entry{act}{article}{}
    \name{author}{1}{}{%
      {{hash=GA}{%
         family={{Graves}},
         familyi={G\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{Computer Science - Neural and Evolutionary Computing}
    \strng{namehash}{GA1}
    \strng{fullhash}{GA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2016}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \verb{eprint}
    \verb 1603.08983
    \endverb
    \field{title}{{Adaptive Computation Time for Recurrent Neural Networks}}
    \field{journaltitle}{ArXiv e-prints}
    \field{eprinttype}{arXiv}
    \field{month}{03}
    \field{year}{2016}
  \endentry

  \entry{machine-learning}{book}{}
    \name{author}{1}{}{%
      {{hash=MT}{%
         family={Mitchell},
         familyi={M\bibinitperiod},
         given={Tom},
         giveni={T\bibinitperiod},
      }}%
    }
    \strng{namehash}{MT1}
    \strng{fullhash}{MT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1997}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{title}{Machine Learning}
    \field{year}{1997}
  \endentry

  \entry{alphago}{article}{}
    \name{author}{17}{}{%
      {{hash=SD}{%
         family={Silver},
         familyi={S\bibinitperiod},
         given={David},
         giveni={D\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Schrittwieser},
         familyi={S\bibinitperiod},
         given={Julian},
         giveni={J\bibinitperiod},
      }}%
      {{hash=SK}{%
         family={Simonyan},
         familyi={S\bibinitperiod},
         given={Karen},
         giveni={K\bibinitperiod},
      }}%
      {{hash=AI}{%
         family={Antonoglou},
         familyi={A\bibinitperiod},
         given={Ioannis},
         giveni={I\bibinitperiod},
      }}%
      {{hash=HA}{%
         family={Huang},
         familyi={H\bibinitperiod},
         given={Aja},
         giveni={A\bibinitperiod},
      }}%
      {{hash=GA}{%
         family={Guez},
         familyi={G\bibinitperiod},
         given={Arthur},
         giveni={A\bibinitperiod},
      }}%
      {{hash=HT}{%
         family={Hubert},
         familyi={H\bibinitperiod},
         given={Thomas},
         giveni={T\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Baker},
         familyi={B\bibinitperiod},
         given={Lucas},
         giveni={L\bibinitperiod},
      }}%
      {{hash=LM}{%
         family={Lai},
         familyi={L\bibinitperiod},
         given={Matthew},
         giveni={M\bibinitperiod},
      }}%
      {{hash=BA}{%
         family={Bolton},
         familyi={B\bibinitperiod},
         given={Adrian},
         giveni={A\bibinitperiod},
      }}%
      {{hash=CY}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={Yutian},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=LT}{%
         family={Lillicrap},
         familyi={L\bibinitperiod},
         given={Timothy},
         giveni={T\bibinitperiod},
      }}%
      {{hash=HF}{%
         family={Hui},
         familyi={H\bibinitperiod},
         given={Fan},
         giveni={F\bibinitperiod},
      }}%
      {{hash=SL}{%
         family={Sifre},
         familyi={S\bibinitperiod},
         given={Laurent},
         giveni={L\bibinitperiod},
      }}%
      {{hash=DGvd}{%
         family={Driessche},
         familyi={D\bibinitperiod},
         given={George van\bibnamedelima den},
         giveni={G\bibinitperiod\bibinitdelim v\bibinitperiod\bibinitdelim
  d\bibinitperiod},
      }}%
      {{hash=GT}{%
         family={Graepel},
         familyi={G\bibinitperiod},
         given={Thore},
         giveni={T\bibinitperiod},
      }}%
      {{hash=HD}{%
         family={Hassabis},
         familyi={H\bibinitperiod},
         given={Demis},
         giveni={D\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Nature Publishing Group}%
    }
    \strng{namehash}{SD+1}
    \strng{fullhash}{SDSJSKAIHAGAHTBLLMBACYLTHFSLDGvdGTHD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2017}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    A long-standing goal of artificial intelligence is an algorithm that
  learns, tabula rasa, superhuman proficiency in challenging domains. Recently,
  AlphaGo became the first program to defeat a world champion in the game of
  Go. The tree search in AlphaGo evaluated positions and selected moves using
  deep neural networks. These neural networks were trained by supervised
  learning from human expert moves, and by reinforcement learning from
  self-play. Here we introduce an algorithm based solely on reinforcement
  learning, without human data, guidance or domain knowledge beyond game rules.
  AlphaGo becomes its own teacher: a neural network is trained to predict
  AlphaGo’s own move selections and also the winner of AlphaGo’s games.
  This neural network improves the strength of the tree search, resulting in
  higher quality move selection and stronger self-play in the next iteration.
  Starting tabula rasa, our new program AlphaGo Zero achieved superhuman
  performance, winning 100–0 against the previously published,
  champion-defeating AlphaGo.%
    }
    \verb{doi}
    \verb 10.1038/nature24270
    \endverb
    \field{issn}{0028-0836}
    \field{number}{7676}
    \field{pages}{354\bibrangedash 359}
    \field{title}{Mastering the game of Go without human knowledge}
    \verb{url}
    \verb http:https://doi.org/10.1038/nature24270
    \endverb
    \field{volume}{550}
    \field{journaltitle}{Nature}
    \field{month}{10}
    \field{year}{2017}
  \endentry

  \entry{no-free-lunch}{article}{}
    \name{author}{1}{}{%
      {{hash=WDH}{%
         family={Wolpert},
         familyi={W\bibinitperiod},
         given={David\bibnamedelima H.},
         giveni={D\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
    }
    \strng{namehash}{WDH1}
    \strng{fullhash}{WDH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1996}
    \field{sortinit}{W}
    \field{sortinithash}{W}
    \field{abstract}{%
    This is the first of two papers that use off-training set (OTS) error to
  investigate the assumption-free relationship between learning algorithms.
  This first paper discusses the senses in which there are no a priori
  distinctions between learning algorithms. (The second paper discusses the
  senses in which there are such distinctions.) In this first paper it is
  shown, loosely speaking, that for any two algorithms A and B, there are “as
  many” targets (or priors over targets) for which A has lower expected OTS
  error than B as vice versa, for loss functions like zero-one loss. In
  particular, this is true if A is cross-validation and B is
  “anti-cross-validation” (choose the learning algorithm with largest
  cross-validation error). This paper ends with a discussion of the
  implications of these results for computational learning theory. It is shown
  that one cannot say: if empirical misclassification rate is low, the
  Vapnik-Chervonenkis dimension of your generalizer is small, and the training
  set is large, then with high probability your OTS error is small. Other
  implications for “membership queries” algorithms and “punting”
  algorithms are also discussed.%
    }
    \verb{doi}
    \verb 10.1162/neco.1996.8.7.1341
    \endverb
    \verb{eprint}
    \verb https://doi.org/10.1162/neco.1996.8.7.1341
    \endverb
    \field{number}{7}
    \field{pages}{1341\bibrangedash 1390}
    \field{title}{The Lack of A Priori Distinctions Between Learning
  Algorithms}
    \verb{url}
    \verb https://doi.org/10.1162/neco.1996.8.7.1341
    \endverb
    \field{volume}{8}
    \field{journaltitle}{Neural Computation}
    \field{year}{1996}
  \endentry
\endsortlist
\endinput
