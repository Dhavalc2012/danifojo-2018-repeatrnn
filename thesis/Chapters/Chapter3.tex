%!TEX root = ../main.tex
% Chapter Template

\chapter{Introduction to Deep Learning} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

Deep learning is the family of machine learning algorithms that use Neural Networks to approximate the target function, and learn using some form of gradient descent via backpropagation.  Neural Networks are called \textbf{networks} because the are made by composing many functions: \(f(x) = f^{(m)}(...f^{(2)}(f^{(1)}(f^{(0)}(x))))\). These allows the network to capture non-linearities, as supposed to simple linear regressions. Each of the functions is called \textbf{layer}, and the amount of layers is called the \textbf{depth} of the network. Usually, each layer of the network is vector-valued. The dimension of each layer is called \textbf{width}. Another way of thinking about these layers is to think of them as many \textbf{units}, each of them representing a vector-to-scalar function.

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
\section{Feedforward Neural Networks}

Feedforward Neural Networks, also called Multilayer Perceptrons (MLP) are the most classical example of Deep Learning. Their goal is to approximate a function \(f(x)\). They are called feedforward because information (\(x\)) only goes forward in the network. A neural network in which there are feedback connections is called Recurrent Neural Network. 


In the case of Feedforward Neural Networks, each of the layers is composed of an affine transformation \(\phi(x) = Wx + b\) and a non-linear function, such as a Gaussian (\(h(x) = \exp(-x^{2}\)) or a sigmoid (\(\sigma(x) = \frac{1}{1+e^{-x}}\)). The non-linear function is called the \textbf{activation}. Then, each layer is of the form: 
\begin{equation}
f^{(i)}(x) = h\left(W^{(i)}x + b^{(i)}\right)
\end{equation}
The activation functions allow the network to capture non-linearities. Here, the learnable parameters are the weights of the affine transformation in each layer.

%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\subsection{XOR}

Nunc posuere quam at lectus tristique eu ultrices augue venenatis. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Aliquam erat volutpat. Vivamus sodales tortor eget quam adipiscing in vulputate ante ullamcorper. Sed eros ante, lacinia et sollicitudin et, aliquam sit amet augue. In hac habitasse platea dictumst.

%-----------------------------------
%	SUBSECTION 2
%-----------------------------------

\subsection{Subsection 2}
Morbi rutrum odio eget arcu adipiscing sodales. Aenean et purus a est pulvinar pellentesque. Cras in elit neque, quis varius elit. Phasellus fringilla, nibh eu tempus venenatis, dolor elit posuere quam, quis adipiscing urna leo nec orci. Sed nec nulla auctor odio aliquet consequat. Ut nec nulla in ante ullamcorper aliquam at sed dolor. Phasellus fermentum magna in augue gravida cursus. Cras sed pretium lorem. Pellentesque eget ornare odio. Proin accumsan, massa viverra cursus pharetra, ipsum nisi lobortis velit, a malesuada dolor lorem eu neque.

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Convolutional Neural Networks}

Sed ullamcorper quam eu nisl interdum at interdum enim egestas. Aliquam placerat justo sed lectus lobortis ut porta nisl porttitor. Vestibulum mi dolor, lacinia molestie gravida at, tempus vitae ligula. Donec eget quam sapien, in viverra eros. Donec pellentesque justo a massa fringilla non vestibulum metus vestibulum. Vestibulum in orci quis felis tempor lacinia. Vivamus ornare ultrices facilisis. Ut hendrerit volutpat vulputate. Morbi condimentum venenatis augue, id porta ipsum vulputate in. Curabitur luctus tempus justo. Vestibulum risus lectus, adipiscing nec condimentum quis, condimentum nec nisl. Aliquam dictum sagittis velit sed iaculis. Morbi tristique augue sit amet nulla pulvinar id facilisis ligula mollis. Nam elit libero, tincidunt ut aliquam at, molestie in quam. Aenean rhoncus vehicula hendrerit.

%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------

\section{Recurrent Neural Networks}

\textbf{Recurrent Neural Networks} or \textbf{RNN} \parencite{rumelhart}, are a family of networks specialized in processing sequential data (\(x^{(1)}, x^{(2)}, \ldots, x^{(T)}\)). Most RNN can also process sequences of variable length. We will refer to RNN acting on a sequence of vectors \(x^{t}\) with \(t\) going from \(1\) to \(T\).

To understand RNN, consider a classical dynamical system driven by an external signal \(x^{(t)}\), in which the state at each timestep is \(s^{t}\):
\begin{equation} \label{eq:rnn}
s^{(t)} = f\left(s^{(t-1)}, x^{(t)}; \theta\right)
\end{equation}

RNN generally take the same form as \ref{eq:rnn}, where the state contains information for the past sequence. The state is generally referred as the \textbf{hidden state}. 

The most simple example of an RNN would be:

\begin{equation} \label{eq:rnn}
s^{(t)} = h\left(W_x x^{(t)} + W_s s^{(t)} + b\right)
\end{equation}

A typical choice for the activation function is \(h(x) = \tanh(x)\).

%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\subsection{Long Short Term Memory}


%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------

\section{Backpropagation}
